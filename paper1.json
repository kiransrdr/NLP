{
    "abstract":"Brain tumor segmentation plays an important role in the disease diagnosis. In this paper, we proposed deep learning frameworks, i.e. MvNet and SPNet, to address the challenges of multimodal brain tumor segmentation. The proposed multi-view deep learning framework (MvNet) uses three multi-branch fully-convolutional residual networks (Mb-FCRN) to segment multimodal brain images from different view-point, i.e. slices along x, y, z axis. The three sub-networks produce independent segmentation results and vote for the final outcome. The SPNet is a CNN-based framework developed to predict the survival time of patients. The proposed deep learning frameworks was evaluated on BraTS 17 validation set and achieved competing results for tumor segmentation While Dice scores of 0.88, 0.75 0.71 were achieved for whole tumor, enhancing tumor and tumor core, respectively, an accuracy of 0.55 was obtained for survival prediction.",
    "introduction": "Brain tumor is a severe disease threating the health of human-being. An accurate automatic tumor segmentation framework can significantly improve the efficiency of disease diagnosis and help design appropriate treatment strategy. In recent years, we witnessed the development of deep learning algorithm and were impressed by its powerful performance. Increasing numbers of studies tried to employ deep learning algorithm to process medical images. In previous challenges, i.e. BraTS 15–16 [1–3], various 2D and 3D deep learning networks have been proposed for the segmentation of multimodal brain tumor. For example, Lun et al. evaluated three types of 2-D convolutional networks, i.e. Patch-Wise, FCN [4] and SegNet [5] for BraTS 15 dataset. Kamnitsas et al. extended a 3-D CNN architecture, i.e. DeepMedic [6], with residualconnections for brain lesion segmentation. In more recent research, Havaei et al. spli 3D brain MRI data into 2D slices and crop patches from 2D slices to train deep neural networks [7]. Fidon et al. proposed scalable multimodal convolutional networks for brain tumor segmentation, which can explicitly leverage deep features within or across modalities [8]. Tseng et al. proposed a Cross-Modality convolutional network for brain tumor segmentation [9]. The network utilizes convolutional LSTM to model a",
	"methodology":"Survival prediction is a new task introduced in BraTS 17. We developed a convolutional network, namely SPNet, to predict the survival days of patients. The convolutional network (CNN) takes the four brain image modals and the tumor segmentation result from MvNet as input and finally predicts the survival days. Figure 5 shows the flowchart of SPNet. The blue, red, orange and purple rectangles represent the convolutional layer, max pooling layer, average pooling layer and fully-connected layer, respectively. The framework concatenates the age information provided by challenge organizers and the features extracted by convolutions for final prediction of survival days. Batch Normalization layer and LeakyReLU are placed after each convolutional layer. The SPNet is optimized by the supervision of Mean Square Error Loss. The ground truths of survival days are normalized to [0, 1] by dividing the maximum value of days for SPNet training, which needs to be multiplied back to the prediction of SPNet during testing",
    "results":"Multimodal Brain Tumor Segmentation Challenge (BraTS) was continuously held since 2012. The datasets utilized in this year have been updated with more routine clinically-acquired 3T multimodal MRI scans and all the ground truth labels have been manually-labelled by expert board-certified neuroradiologists. As illustrated in Fig. 2, four modal brain MRI scans, i.e. native (T1), post-contrast T1-weighted (T1Gd),T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes, were acquired. The resolution of MRI scans is 240  240  155. The BraTS 17 training dataset contains 210 multimodal MRI scans of glioblastoma (GBM/HGG) and 75 scans of lower grade glioma (LGG). The BraTS 17 validation set contains 46 MRI scans. The competition participants need to test their algorithms on the validation set and send the segmentation results to the challenge organizers for evaluation. All the imaging datasets have been segmented manually, by one to four raters, following the same annotation protocol, and their annotations were approved by experienced neuro-radiologists. Annotations comprise the GD-enhancing tumor (ET  label 4), the peritumoral edema (ED — label 2), and the necrotic and non-enhancing tumor (NCR/NET — label 1), as described in [1]. The annotations of BraTS 17 training set is provided to the comp",
	"conclusion":"In this paper, we proposed deep learning frameworks, so-called MvNet and SPNet, to address the tasks of multi-modal brain tumor segmentation and survival prediction of patients. The proposed MvNet employs three sub-networks to process the brain images along different axis. The results on BraTS 17 validation set show the competing performance of MvNet, i.e. Dice scores of 0.88, 0.75 and 0.71 were achieved for whole tumor, enhancing tumor and tumor core, respectively. The SPNet is a CNN-based network, which achieved an accuracy of 0.55 on BraTS 17 validation set."

	}